Analyses for the paper: "A 365 Participants Corpus of Eye Movements in Learner and Native English Reading"

Before running this script please obtain the data as follows:

CELER:
1. Obtain the [PTB-WSJ](https://catalog.ldc.upenn.edu/LDC95T7) and [BLLIP](https://catalog.ldc.upenn.edu/LDC2000T43) corpora through LDC.
2. - Copy the `README` file of the PTB-WSJ (starts with "This is the Penn Treebank Project: Release 2 ...") to the folder `ptb_bllip_readmes/PTB/`. 
   - Copy the `README.1st` file of BLLIP (starts with "File:  README.1st ...") to the folder `ptb_bllip_readmes/BLLIP/`
3. Run `python obtain_data.py` to download `data_2.0.zip`. Extract to the top level of this directory.

GECO:
Download GECO augmented with frequency and surprisal values from the following url and place `geco/` at the top level of this directory
https://drive.google.com/file/d/1T4qgbwPkdzYmTvIqMUGJlvY-v22Ifinx/view?usp=sharing

```{r}

#TODO: change CELER path

set.seed(214)

library(tidyverse)
library(lme4)
library(lmerTest)
library(readxl)
library(car) #for Levene test
library(JuliaCall)

j<-julia_setup()
j$library("MixedModels")

DATA_VERSION = 0 #0 for entire dataset, 1 for v1, 2 for subjects new to v2
```

Load CELER and GECO metadata and Plot basic participant stats
```{r,fig.width=12,fig.height=4}
geco_metadata = read_xlsx("geco/SubjectInformation.xlsx", na = c("", ".")) %>% rename("Age" = "AGE", "English.AoA" =  "AOA_ENG") %>% mutate(English = ifelse(GROUP == "bilingual", "L2", "L1"),
                        L1 = as.factor(ifelse(GROUP == "bilingual", "Dutch", "English")),
                        DATASET = "GECO")

celer_metadata = read.table("participant_metadata/metadata.tsv", header = TRUE, quote = "", sep = "\t") %>% mutate(English = ifelse(L1 == "English", "L1", "L2"),
                        English.AoA =as.numeric(as.character(replace(English.AoA, English.AoA == "at birth", 0))),
                        DATASET = "CELER") %>%
                        column_to_rownames(var="List")

metadata_all = bind_rows(geco_metadata, celer_metadata) %>% mutate_at(c("English", "L1", "DATASET"), as.factor)
metadata_long = gather(metadata_all, "property", "x", c("Age", "English.AoA",  "MichiganLG"), factor_key = TRUE)


property.labs = c("Age", "English Age of Acquisition", "MichiganLG")
names(property.labs) = c("Age", "English.AoA", "MichiganLG")
plot_data = filter(metadata_long, English == "L2")
p <- ggplot() +
      theme_minimal(base_size = 40) +
      geom_histogram(data = plot_data, aes(x = x, fill = DATASET), binwidth = 4, alpha = 0.7) +
      facet_grid(. ~ property, scales = "free", labeller = labeller(property = property.labs)) +
      scale_color_manual(values = c("CELER"="blue", "GECO"="red")) +
      scale_fill_manual(values = c("CELER"="blue", "GECO"="red")) +
      theme(aspect.ratio=1) +
      labs(y = "# ESL Participants",
           x = NULL,
           fill = "Dataset")
    ggsave(file="dataset_analyses_figures/participants.png", p)
p
```


```{r}
code_native <- function(report){
    report$L2_01 <- ifelse(report$English == "L2", 1, 0)
    report$L2_05 <- ifelse(report$English == "L2", 0.5, -0.5)
    report$L2_11 <- ifelse(report$English == "L2", 1, -1)
    return(report)
}
```

Load GECO
```{r}
geco_ia_l2 <- read_xlsx("geco/L2ReadingDataAugmented.xlsx", na = c("", ".")) 
geco_ia_l1 <- read_xlsx("geco/MonolingualReadingDataAugmented.xlsx", na = c("", "."))
geco_ia <- bind_rows(geco_ia_l1, geco_ia_l2) %>% 
           mutate(GROUP = as.factor(GROUP),
                  PP_NR = as.factor(PP_NR),
                  WORD_ID = as.factor(WORD_ID),
                  WORD_NORM = as.factor(WORD_NORM),
                  LANGUAGE_RANK = as.factor(LANGUAGE_RANK),
                  #WORD_TOTAL_READING_TIME = ifelse(is.na(WORD_TOTAL_READING_TIME), 0, WORD_TOTAL_READING_TIME),
                  FREQ_BLLIP = as.double(FREQ_BLLIP),
                  FREQ_WEB = as.double(FREQ_WEB),
                  FREQ_SUBTLEX = as.double(FREQ_SUBTLEX)) %>% 
           rename(SUBJECT = PP_NR,
                  English = LANGUAGE_RANK,
                  FIRST_FIXATION = WORD_FIRST_FIXATION_DURATION,
                  GAZE_DURATION = WORD_GAZE_DURATION,
                  TOTAL_FIXATION = WORD_TOTAL_READING_TIME,
                  AVERAGE_FIX_PUPIL_SIZE = WORD_AVERAGE_FIX_PUPIL_SIZE,
                  FIXATION_COUNT = WORD_FIXATION_COUNT,
                  SKIP = WORD_SKIP) %>% 
           mutate(FREQ = FREQ_SUBTLEX, #set default frequency to SUBTLEX
                  OOV = OOV_SUBTLEX,
                  #WORD_LEN = nchar(IA_LABEL), #This is precomputed excluding punctuation
                  shared_text = 1,
                  DATASET = "GECO") %>%                          
           code_native
geco_ia_long <- gather(geco_ia, "fix_measure", "RT", 
                    c('FIRST_FIXATION', 'GAZE_DURATION', 'TOTAL_FIXATION', 'SKIP', 'FIXATION_COUNT'), factor_key = TRUE) 
```


Load CELER Interest area report
```{r}
#Load interest area report
celer_ia <- read.table("data_v2.0/sent_ia.tsv", header = TRUE, quote = "", sep = "\t", na.strings = c("", ".")) 

if (DATA_VERSION %in% c(1,2)){
  celer_ia <- filter(celer_ia, dataset_version == DATA_VERSION)
}

celer_ia <- celer_ia %>%
                 #rename variables
                 rename(SUBJECT = list,
                        TRIAL = trial,
                        FIRST_FIXATION = IA_FIRST_FIXATION_DURATION,
                        GAZE_DURATION = IA_FIRST_RUN_DWELL_TIME,
                        TOTAL_FIXATION = IA_DWELL_TIME,
                        SKIP = IA_SKIP,
                        FIXATION_COUNT = IA_FIXATION_COUNT,
                        WORD = IA_LABEL) %>%
                 mutate(DATASET = "CELER",
                        # set unfixated word reading times to 0
                        #FIRST_FIXATION = as.integer(replace(as.character(FIRST_FIXATION), FIRST_FIXATION == ".", "0")),
                        #GAZE_DURATION = as.integer(replace(as.character(GAZE_DURATION), GAZE_DURATION == ".", "0")),
                        TOTAL_FIXATION = na_if(TOTAL_FIXATION, 0),
                        # set default frequency 
                        FREQ = FREQ_SUBTLEX, 
                        OOV = OOV_SUBTLEX,
                        # subjects and words as factors
                        SUBJECT = as.factor(SUBJECT),
                        WORD_ID = as.factor(paste(TRIAL, IA_ID, sep = "_")), #meaninfull only for the shared regime
                        WORD_NORM = as.factor(WORD_NORM),    
                        WORD = as.character(WORD),
                        dataset_version = as.factor(dataset_version),
                        # add L1 and proficiency information form metadata
                        MPT = map_dbl(SUBJECT, function(x){celer_metadata[toString(x),"MichiganLG"]}),
                        L1 = unlist(map(SUBJECT, function(x){celer_metadata[toString(x),"L1"]})),
                        English = as.factor(ifelse(L1 == "English", "L1", "L2"))) %>% 
                 code_native

celer_ia_long <- gather(celer_ia, "fix_measure", "RT", 
                        c('FIRST_FIXATION', 'GAZE_DURATION', 'TOTAL_FIXATION', "SKIP", "FIXATION_COUNT"), factor_key = TRUE) 

#compute approximate number of characters per visual angle
n_upper <- sum(str_count(celer_ia$WORD, "[A-Z]"), na.rm = TRUE)
n_all_char <- sum(nchar(celer_ia$WORD), na.rm = TRUE)
n_lower <- n_all_char - n_upper

#0.36 degrees for lowercase letter, 0.49 for uppercase
mean_char_visual_degrees = 0.36*(n_lower/n_all_char)+0.49*(n_upper/n_all_char) 
CHARS_PER_ANGLE = 1/mean_char_visual_degrees
```


Check basic stats (not in paper)
```{r}
celer_ia %>% group_by(dataset_version, English) %>% summarise_at(vars('FIRST_FIXATION', 'GAZE_DURATION', 'TOTAL_FIXATION'), max, na.rm=TRUE)

ggplot() +
  geom_histogram(data = filter(celer_ia, TOTAL_FIXATION > 10000), aes(x = TOTAL_FIXATION, color = dataset_version, fill = dataset_version))

```

Load CELER Fixation report

```{r}
#Load Fixation report
celer_fix <- read.table("data_v2.0/sent_fix.tsv", header = TRUE, quote = "", sep = "\t", stringsAsFactors = FALSE) 

if (DATA_VERSION %in% c(1,2)){
  celer_fix <- filter(celer_fix, dataset_version == DATA_VERSION)
}

celer_fix <- celer_fix %>%
                  #filter out saccades to and from locations that outside the text area
                  filter(NEXT_SAC_AMPLITUDE != '.', 
                         CURRENT_FIX_INTEREST_AREA_ID != '.') %>% 
                  rename(SUBJECT = list,
                         TRIAL = trial) %>%
                  # add L1 and proficiency information form metadata
                  mutate(MPT = map_dbl(SUBJECT, function(x){celer_metadata[toString(x),"MichiganLG"]}),
                         L1 = unlist(map(SUBJECT, function(x){celer_metadata[toString(x),"L1"]})),
                         English = as.factor(ifelse(L1 == "English", "L1", "L2"))) %>% 
                  code_native %>% 
                  mutate(DATASET = "CELER",
                         #set default frequency
                         FREQ = FREQ_BLLIP, 
                         OOV = OOV_BLLIP, 
                         SUBJECT = as.factor(SUBJECT),
                         WORD_ID = as.factor(paste(TRIAL, CURRENT_FIX_INTEREST_AREA_ID, sep = "_")),
                         WORD_NORM = as.factor(WORD_NORM),    #WORD = as.character(WORD),
                         REGRESSION = ifelse(NEXT_SAC_DIRECTION == "LEFT", 1, 0),
                         NEXT_SAC_AMPLITUDE = as.double(NEXT_SAC_AMPLITUDE)*CHARS_PER_ANGLE,
                         NEXT_SAC_AVG_VELOCITY = as.double(NEXT_SAC_AVG_VELOCITY)*CHARS_PER_ANGLE)

celer_fix_long <- gather(celer_fix, "fix_measure", "RT", 
                         c("CURRENT_FIX_DURATION", "NEXT_SAC_AMPLITUDE", "NEXT_SAC_AVG_VELOCITY", "REGRESSION"), factor_key = TRUE) 
```
TODO?: can also filter NEXT_FIX_INTEREST_AREA_ID != '.' to exclude saccades that go outside the text region

Preprocess data
```{r}
filter_words <- function(df){
      df_filtered <- df %>% #first and last words
                            group_by(SUBJECT, TRIAL) %>% 
                            slice(2:(n()-1)) %>% ungroup() %>% 
                            filter(!grepl("NUM", WORD_NORM), #numbers
                                   !grepl('^[[:punct:]]|[[:punct:]]$', WORD), #words with punctuation
                                   (OOV == 0), #out of vocabulary words
                                   !is.na(TOTAL_FIXATION)) #skips

  return(df_filtered)
}

center_predictors <- function(df){
  df_centered <- df %>% mutate(FREQ = FREQ - mean(FREQ), 
                               SURP_GPT2 = SURP_GPT2 - mean(SURP_GPT2), 
                               WORD_LEN = WORD_LEN- mean(WORD_LEN)) 
  return(df_centered)
}

celer_ia_oov <- celer_ia %>% group_by(shared_text) %>% filter_words %>% center_predictors %>% ungroup()
celer_ia_oov_long <- gather(celer_ia_oov, "fix_measure", "RT", 
                            c("FIRST_FIXATION", "GAZE_DURATION", "TOTAL_FIXATION"))
geco_ia_oov <- geco_ia %>% filter_words %>% center_predictors
geco_ia_oov_long <- gather(geco_ia_oov, "fix_measure", "RT", 
                        c("FIRST_FIXATION", "GAZE_DURATION", "TOTAL_FIXATION"))
```

Table 2: Benchmarks for standard fixation measures for L1 and L2 on CELER and GECO
```{r}
mean_lmer <- function(report){
  shared_text = unique(report$shared_text)
  if (shared_text == 1){
      print(paste("shared", unique(report$fix_measure), unique(report$English)))
      se = as_tibble(coef(summary(lmer(RT ~ 1 + (1 |SUBJECT) + (1|WORD_ID), # 
                                       control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data = report))))
  }else{
      print(paste("individual", unique(report$fix_measure), unique(report$English)))
      se = as_tibble(coef(summary(lmer(RT ~ 1 + (1 |SUBJECT),
                                       control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE),data = report))))
  }
  se <- se %>% mutate(CI = `Std. Error`*1.96)
  return(se)    
}

run_lmer <- function(report){
  
  shared_text = unique(report$shared_text)
  measure = unique(report$fix_measure)
  
  if ((measure == "SKIP")|(measure == "REGRESSION")){
      if (shared_text == 1){
          print(paste("MODEL logistic", "shared", measure))
          m <- glmer(RT ~ L2_01 + (1 |SUBJECT) + (L2_01|WORD_ID), family = binomial(), 
                     control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=report)
      }else{
          print(paste("MODEL logistic", "individual", measure))
          m <- glmer(RT ~ L2_01 + (1 |SUBJECT), family = binomial(), 
                     control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=report)
      }
  } else if (measure == "FIXATION_COUNT"){
      if (shared_text == 1){
          print(paste("MODEL poisson", "shared", measure))
          m <- glmer(RT ~ L2_01 + (1 |SUBJECT) + (L2_01|WORD_ID), family = poisson(), 
                     control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=report)
      }else{
          print(paste("MODEL poisson", "individual", measure))
          m <- glmer(RT ~ L2_01 + (1 |SUBJECT), family = poisson(), # 
                     control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=report)

      }
    }
  else{
      if (shared_text == 1){
          print(paste("MODEL regression", "shared", measure))
          m <- lmer(RT ~ L2_01 + (1 |SUBJECT) + (L2_01|WORD_ID), 
                    control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=report)
      }else{
          print(paste("MODEL regression", "individual", measure))
          m <- lmer(RT ~ L2_01 + (1 |SUBJECT), # 
                    control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=report)
      }
    }
  fixed_effects <- data.frame(summary(m)$coefficients)
  return(fixed_effects)
}
```

```{r}
means_celer_ia <- celer_ia_long %>% group_by(shared_text, fix_measure, English) %>% do(mean_lmer(.))
tests_celer_ia <- celer_ia_long %>% group_by(shared_text, fix_measure) %>% do(run_lmer(.))
```

```{r}
means_celer_ia %>% mutate_if(is.numeric, round, digits=1)
tests_celer_ia %>% mutate_if(is.numeric, round, digits=1)
```

```{r}
means_geco <- geco_ia_long %>% group_by(shared_text, fix_measure, English) %>% do(mean_lmer(.))
tests_geco <- geco_ia_long %>% group_by(shared_text, fix_measure) %>% do(run_lmer(.))
```

```{r}
means_geco %>% mutate_if(is.numeric, round, digits=1)
tests_geco %>% mutate_if(is.numeric, round, digits=1)
```

```{r}
means_celer_fix <- celer_fix_long %>% group_by(shared_text, fix_measure, English) %>% do(mean_lmer(.))
tests_celer_fix <- celer_fix_long %>% group_by(shared_text, fix_measure) %>% do(run_lmer(.))
```

```{r}
means_celer_fix %>% mutate_if(is.numeric, round, digits=2)
tests_celer_fix %>% mutate_if(is.numeric, round, digits=2)
```


```{r}
celer_fix <- celer_fix %>% mutate(NEXT_SAC_AMPLITUDE_c = NEXT_SAC_AMPLITUDE - mean(NEXT_SAC_AMPLITUDE))
m_velocity <- lmer(NEXT_SAC_AVG_VELOCITY ~ L2_05*NEXT_SAC_AMPLITUDE_c + (1 |SUBJECT) + (L2_05|WORD_ID), 
                   control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=celer_fix)
summary(m_velocity)$coefficients
```


Table 3: the effects of frequency, surprisal and word length on reading times (First Fixation, Gaze Duration, Total fixation)

```{r}
get_fixed_effects_julia <- function(report, form){
    julia_assign("report", report)
    julia_assign("formula", formula(form))
    result <- julia_eval("m1 = coeftable(fit(LinearMixedModel, formula, report))")
    return(result)
}
```

CELER
```{r}
for (measure in c("FIRST_FIXATION", "GAZE_DURATION", "TOTAL_FIXATION")){
      print(measure)
      report = filter(celer_ia_oov_long, fix_measure == measure)
      formula1 = "RT ~ English/FREQ + English/SURP_GPT2 + English/WORD_LEN + (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT)"
      print(get_fixed_effects_julia(report, formula1))
      cat("\n")
      formula2 = "RT ~ L2_05*FREQ + L2_05*SURP_GPT2 + L2_05*WORD_LEN + (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT)"
      print(get_fixed_effects_julia(report, formula2))
      cat("\n")
}
```

GECO
```{r}
for (measure in c("FIRST_FIXATION", "GAZE_DURATION", "TOTAL_FIXATION")){
      print(measure)
      report = filter(geco_ia_oov_long, fix_measure == measure)
      formula1 = "RT ~ English/FREQ + English/SURP_GPT2 + English/WORD_LEN + (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT) + (1 | WORD_ID)"
      print(get_fixed_effects_julia(report, formula1))
      cat("\n")
      formula2 = "RT ~ L2_05*FREQ + L2_05*SURP_GPT2 + L2_05*WORD_LEN + (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT) + (English | WORD_ID)"
      print(get_fixed_effects_julia(report, formula2))
      cat("\n")
}
```


```{r}
get_fixed_effects <- function(report, parametrization){
  
      shared_text = unique(report$shared_text)
      dataset_name = unique(report$DATASET)
      measure = unique(report$fix_measure)
      
      print(paste(dataset_name, measure))
      if(dataset_name == "CELER"){
          print("CELER")
          if(parametrization == "nested"){
              m <- lmer(RT ~ English/FREQ + English/SURP_GPT2 + English/WORD_LEN + 
                        (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT) + (SURP_GPT2 | WORD_NORM), 
                         control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data = report)
          }else{
              m <- lmer(RT ~ L2_05*FREQ + L2_05*SURP_GPT2 + L2_05*WORD_LEN + 
                        (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT) + (SURP_GPT2| WORD_NORM),# +  
                        control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=report) #
          }
      }else{
        print("GECO")

        if(parametrization == "nested"){
              m <- lmer(RT ~ English/FREQ + English/SURP_GPT2 + English/WORD_LEN + 
                        (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT) + (1 | WORD_ID), #
                        control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data = report)
          }else{
              m <- lmer(RT ~ L2_05*FREQ + L2_05*SURP_GPT2 + L2_05*WORD_LEN + 
                        (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT) + (1 | WORD_ID), #
                        control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data=report)
          }
      }
      coefs <- data.frame(summary(m)$coefficients) %>% rownames_to_column("COEF") %>% mutate(CI = Std..Error*1.96)
      return(coefs)
}
```

```{r}
#coefs_celer <- filter(celer_ia_oov_long, shared_text == 0, fix_measure == "FIRST_FIXATION") %>% group_by(DATASET) %>% do(get_fixed_effects(., "nested"))

# m <- lm(RT ~ English/FREQ + English/SURP_GPT2 + English/WORD_LEN, data = filter(celer_ia_oov_long, fix_measure == "FIRST_FIXATION"))
#coefs <- data.frame(summary(m)$coefficients) %>% rownames_to_column("COEF") %>% mutate(CI = Std..Error*1.96)
#print(coefs)
#coefs_celer <- filter(celer_ia_oov_long, fix_measure == "IA_FIRST_RUN_DWELL_TIME") %>% do(get_fixed_effects(., "nested"))


m <- lmer(RT ~ English/FREQ + English/SURP_GPT2 + English/WORD_LEN + 
                        (FREQ + SURP_GPT2 + WORD_LEN | SUBJECT) + (1 | WORD_ID), #
                        control=lmerControl(optimizer = "bobyqa", calc.derivs = FALSE), data = filter(geco_ia_oov_long, fix_measure == "FIRST_FIXATION"))
c <- data.frame(summary(m)$coefficients) %>% rownames_to_column("COEF") %>% mutate(CI = Std..Error*1.96)
print(c)

```


```{r}
```
```{r}
coefs
```

```{r}
coefs_celer <- celer_ia_oov_long %>% group_by(DATASET, fix_measure) %>% do(get_fixed_effects(., "nested"))
coefs_geco <- geco_ia_oov_long %>% group_by(DATASET, fix_measure) %>% do(get_fixed_effects(., "nested"))

```

Print coefficients
```{r}
print(coefs_geco) %>% mutate_if(is.numeric, round, digits=1)
print(coefs_celer) %>% mutate_if(is.numeric, round, digits=1)
```
```

Interactions of word propety effect and English status
```{r}
inter_geco <- geco_ia_oov_long %>% group_by(DATASET, fix_measure) %>% do(get_fixed_effects(., "unnested"))
inter_celer <- celer_ia_oov_long %>% group_by(DATASET, shared_text, fix_measure) %>% do(get_fixed_effects(., "unnested"))
```

```{r}
inter_geco #%>% mutate_if(is.numeric, round, digits=1)
inter_celer# %>% mutate_if(is.numeric, round, digits=1)
```
Obtain random effects for separately for L1 and L2 in CELER and GECO
```{r}
get_random_effects <- function(dataset){
  print(paste(unique(dataset$DATASET), unique(dataset$English)))
  m <- lmer(IA_DWELL_TIME ~ FREQ + SURP_GPT2 + WORD_LEN + (FREQ + SURP_GPT2+ WORD_LEN ||SUBJECT) , data=dataset) #+ (1|WORD_ID)
  re = ranef(m)
  dev = summary(m)$coefficients[,1]
  re$SUBJECT = sweep(re$SUBJECT, 2, dev, "/") #divide random effects by fixed effects
  coefs = re$SUBJECT %>% rownames_to_column("SUBJECT")
  return(coefs)
}

coefs_geco <- geco_ia_oov %>% group_by(DATASET,English) %>% do(get_random_effects(.)) %>% gather(key=coef, value  = random_to_fixed_ratio, -SUBJECT, -DATASET, -English) %>% ungroup()
coefs_celer_shared <- celer_ia_oov %>% filter(shared_text == 1) %>% group_by(DATASET,English) %>% do(get_random_effects(.)) %>% gather(key=coef, value  = random_to_fixed_ratio, -SUBJECT, -DATASET, -English) %>% ungroup() #
coefs_celer_shared
```

```{r}
coefs_all <- bind_rows(coefs_geco, coefs_celer_shared) %>% mutate(DATASET = as.factor(DATASET),
                                                                  SUBJECT = as.factor(SUBJECT),
                                                                  coef = as.factor(coef))
coefs_all
```

```{r}
library(lattice)
p <- ggplot() +
      theme_bw() +
      geom_point(data = coefs_all, aes(x = coef, y = random_to_fixed_ratio, fill = DATASET), 
              pch = 21, position = position_jitterdodge(dodge.width = 1), alpha = 0.3) + 
      geom_boxplot(data = coefs_all, aes(x = coef, random_to_fixed_ratio, fill = DATASET), width = 0.3, outlier.size = 0) +
      facet_grid(English ~ ., scales = "free")+
      theme(axis.title.x=element_blank())

ggsave(file="dataset_analyses_figures/coefs_tf_shared.pdf", p)#height=20,width=24,
p
```


Test for difference in variances between CELER and GECO (dradt)
```{r}
int_coefs <- coefs_all %>% filter(coef == "SURP_GPT2", English == "L2")
#test = var.test(random_to_fixed_ratio ~ DATASET, data = int_coefs)
test = leveneTest(random_to_fixed_ratio ~ DATASET, data = int_coefs, center = mean)
test
```


```{r}
proficiency_data <- filter(celer_ia_oov, shared_text ==1, English == "L2") %>% mutate(MPT = MPT - mean(MPT))
m <- lmer(IA_DWELL_TIME ~ FREQ*MPT + SURP_GPT2*MPT + WORD_LEN*MPT + (FREQ + SURP_GPT2+ WORD_LEN ||SUBJECT) , data=proficiency_data)
summary(m)
```


THE FOLLOWING IS TESTING FOR DIFFERENT WAYS TO GET ERROR BARS AROUND MEANS (TODO: REMOVE)


L1 mean
```{r}
filter(celer_ia, shared_text == 1, English == "L1") %>% summarize(mean(IA_DWELL_TIME, na.rm = TRUE))
```


```{r}
predict_from_model <- function(model, value){
  c <- summary(model)$coefficients[,1]
  x <- as.matrix(value)
  mean <- c%*%x
  se <- sqrt(t(x)%*%vcov(model)%*%x)
  return(c(mean= mean, SE = drop(se)))
} 
```

These are the model coefficients with both L1 and L2 (-0.5 L1, 0.5 L2)
```{r}
mAll <- lmer(IA_DWELL_TIME ~ L2_05 + (1 |SUBJECT) + (L2_05|WORD_ID), 
           data=filter(celer_ia, shared_text == 1))
summary(mAll)$coefficients
```
Predicting from this model.
```{r}
predict_from_model(mAll, c(1,-05))
```

Intercerept model for L1 participants. Result: standard error much smaller.
```{r}
mL1 <- lmer(IA_DWELL_TIME ~ 1 + (1 |SUBJECT) + (1|WORD_ID), data=filter(celer_ia, shared_text ==1, English == "L1")) #filter(geco_ia, English != "L1"))
summary(mL1)$coefficients
```

Predicting from this model.
```{r}
predict_from_model(mL1, c(1))
```

Raw Skip Rate
```{r}
filter(celer_ia, English == "L1") %>% summarise(mean(IA_SKIP))
```

```{r}
#mL2 <- lmer(IA_SKIP ~ 1 + (1 |SUBJECT) + (1|WORD_ID), data=filter(celer_ia, shared_text ==1, English == "L2"))  
#summary(mg)$coefficients
mL2 <- glm(IA_SKIP ~ 1 + (1|SUBJECT)  + (1 |WORD_ID), family = binomial(), data=filter(celer_ia, shared_text ==1, English == "L2"))
#m <- glmer(IA_SKIP ~ -1 + L2_05 + (-1 + L2_05 | WORD_ID), family = binomial(), data=celer_ia)

```
```{r}
vcov(mL2)
```

```{r}
summary(mL2)$coefficients[1:2]
coef = summary(mL2)$coefficients[1]
se = summary(mL2)$coefficients[2]
coef_logistic = exp(coef)/(1+exp(coef))
coef_logistic
upper = coef+se
upper
upper_prob <-exp(upper)/(1+exp(upper))
upper_prob
upper_prob-coef_logistic
```
```{r}
#mAll <-glm(IA_SKIP ~ L2_05 + (1|SUBJECT) + (L2_05|WORD_ID), family = binomial(), data=filter(celer_ia, shared_text ==1)) 
mAll <-lmer(IA_SKIP ~ L2_05 + (1|SUBJECT) + (L2_05|WORD_ID), data=filter(celer_ia, shared_text ==1)) 
```

```{r}
#summary(mAll)$coefficients[,1]
x <-c(1,0.5)
#predict_from_model(mAll, x)
t(x)
vcov(mAll)
```

```{r}
celer_ia_shared %>% group_by(English) %>% summarise(mean(IA_SKIP))
```


```{r}
m <- lmer(paste("IA_SKIP", "~ L2_05 + (1 |SUBJECT) + (L2_05|WORD_ID)"), data=filter(celer_ia, shared_text ==1))  
summary(m)$coefficients
```

```{r}
vcov(m)
```


```{r}
summary(glmer(paste("IA_FIXATION_COUNT", " ~ 1 + (1 |SUBJECT) + (1|WORD_ID)"), family = poisson(), data = filter(celer_ia, L1 != "English")))$coefficients
```



